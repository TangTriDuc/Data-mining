{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7 - Textual Data Analytics\n",
    "Complete the code with TODO tag.\n",
    "## 1. Feature Engineering\n",
    "In this exercise we will understand the functioning of TF/IDF ranking. Implement the feature engineering and its application, based on the code framework provided below.\n",
    "\n",
    "First we use textual data from Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2819\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>849636868052275200</td>\n",
       "      <td>2017-04-05 14:56:29</td>\n",
       "      <td>b'And so the robots spared humanity ... https:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>848988730585096192</td>\n",
       "      <td>2017-04-03 20:01:01</td>\n",
       "      <td>b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>848943072423497728</td>\n",
       "      <td>2017-04-03 16:59:35</td>\n",
       "      <td>b'@waltmossberg @mims @defcon_5 Et tu, Walt?'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>848935705057280001</td>\n",
       "      <td>2017-04-03 16:30:19</td>\n",
       "      <td>b'Stormy weather in Shortville ...'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>848416049573658624</td>\n",
       "      <td>2017-04-02 06:05:23</td>\n",
       "      <td>b\"@DaveLeeBBC @verge Coal is dying due to nat ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id           created_at  \\\n",
       "0  849636868052275200  2017-04-05 14:56:29   \n",
       "1  848988730585096192  2017-04-03 20:01:01   \n",
       "2  848943072423497728  2017-04-03 16:59:35   \n",
       "3  848935705057280001  2017-04-03 16:30:19   \n",
       "4  848416049573658624  2017-04-02 06:05:23   \n",
       "\n",
       "                                                text  \n",
       "0  b'And so the robots spared humanity ... https:...  \n",
       "1  b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...  \n",
       "2      b'@waltmossberg @mims @defcon_5 Et tu, Walt?'  \n",
       "3                b'Stormy weather in Shortville ...'  \n",
       "4  b\"@DaveLeeBBC @verge Coal is dying due to nat ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv('elonmusk_tweets.csv')\n",
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Text Normalization\n",
    "Now we need to normalize text by stemming, tokenizing, and removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "import pprint \n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['band', 'so', 'the', 'robot', 'spare', 'human', 'httpstcov7jujqwfcv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(document):\n",
    "    # TODO: remove punctuation\n",
    "    text = \"\".join([ch for ch in document if ch not in string.punctuation])\n",
    "    \n",
    "    # TODO: tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # TODO: Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    ret = \" \".join([stemmer.stem(word.lower())for word in tokens])\n",
    "    return ret\n",
    "\n",
    "original_documents = [x.strip() for x in data['text']] \n",
    "documents = [normalize(d).split() for d in original_documents]\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see that the normalization is still not perfect. Please feel free to improve upon (OPTIONAL), e.g. https://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Implement TF-IDF\n",
    "Now you need to implement TF-IDF, including creating the vocabulary, computing term frequency, and normalizing by tf-idf weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tesla', 287)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['brt', 'tesla', 'spacex', 'model', 'thi']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten all the documents\n",
    "flat_list = [word for doc in documents for word in doc]\n",
    "\n",
    "# TODO: remove stop words from the vocabulary\n",
    "words = [word for word in flat_list if word not in stopwords.words('english')]\n",
    "\n",
    "# TODO: we take the 500 most common words only\n",
    "counts = Counter(words)\n",
    "vocabulary = counts.most_common(500)\n",
    "print([x for x in vocabulary if x[0] == 'tesla'])\n",
    "vocabulary = [x[0] for x in vocabulary]\n",
    "assert len(vocabulary) == 500\n",
    "\n",
    "# vocabulary.sort()\n",
    "vocabulary[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['tesla', 'exactli'], dtype='<U17'), array([1, 1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tf(vocabulary, documents):\n",
    "    matrix = [0] * len(documents)\n",
    "    for i, document in enumerate(documents):\n",
    "        counts = Counter(document)\n",
    "        matrix[i] = [0] * len(vocabulary)\n",
    "        for j, term in enumerate(vocabulary):\n",
    "            matrix[i][j] = counts[term]\n",
    "    return matrix\n",
    "\n",
    "tf = tf(vocabulary, documents)\n",
    "np.array(vocabulary)[np.where(np.array(tf[1]) > 0)], np.array(tf[1])[np.where(np.array(tf[1]) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.539126825495932,\n",
       " 3.3163095197385393,\n",
       " 3.7262581423445837,\n",
       " 3.8171115727956972,\n",
       " 3.8027562798186274]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def idf(vocabulary, documents):\n",
    "    \"\"\"TODO: compute IDF, storing values in a dictionary\"\"\"\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        idf[term] = math.log(num_documents / sum(term in document for document in documents),2)\n",
    "    return idf\n",
    "\n",
    "idf = idf(vocabulary, documents)\n",
    "[idf[key] for key in vocabulary[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['tesla', 'exactli'], dtype='<U17'), array([3.31630952, 6.65361284]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize(document, vocabulary, idf):\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term]\n",
    "    return vector\n",
    "\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]\n",
    "np.array(vocabulary)[np.where(np.array(document_vectors[1]) > 0)], np.array(document_vectors[1])[np.where(np.array(document_vectors[1]) > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Compare the results with the reference implementation of scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the scikit-learn library. As you can see that, the way we do text normalization affects the result. Feel free to further improve upon (OPTIONAL), e.g. https://stackoverflow.com/questions/36182502/add-stemming-support-to-countvectorizer-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('http', 163.54366542841234), ('https', 151.85039944652075), ('rt', 112.61998731390989), ('tesla', 95.96401470715628), ('xe2', 88.20944486346477)]\n",
      "testla 0.3495243100660956\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english', max_features=500)\n",
    "\n",
    "features = tfidf.fit(original_documents)\n",
    "corpus_tf_idf = tfidf.transform(original_documents) \n",
    "\n",
    "sum_words = corpus_tf_idf.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in tfidf.vocabulary_.items()]\n",
    "print(sorted(words_freq, key = lambda x: x[1], reverse=True)[:5])\n",
    "print('testla', corpus_tf_idf[1, features.vocabulary_['tesla']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.  Apply TF-IDF for information retrieval\n",
    "We can use the vector representation of documents to implement an information retrieval system. We test with the query $Q$ = \"tesla nasa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 documents\n",
      "0 b'@ashwin7002 @NASA @faa @AFPAA We have not ruled that out.'\n",
      "1 b'RT @NASA: Updated @SpaceX #Dragon #ISS rendezvous times: NASA TV coverage begins Sunday at 3:30amET: http://t.co/qrm0Dz4jPE. Grapple at  ...'\n",
      "2 b\"Deeply appreciate @NASA's faith in @SpaceX. We will do whatever it takes to make NASA and the American people proud.\"\n",
      "3 b'Would also like to congratulate @Boeing, fellow winner of the @NASA commercial crew program'\n",
      "4 b\"@astrostephenson We're aiming for late 2015, but NASA needs to have overlapping capability to be safe. Would do the same\"\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(v1,v2):\n",
    "    \"\"\"TODO: compute cosine similarity\"\"\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y =v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = sumxy/math.sqrt(sumxx*sumyy)\n",
    "    return result\n",
    "\n",
    "def search_vec(query, k, vocabulary, stemmer, document_vectors, original_documents):\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    \n",
    "    # TODO: rank the documents by cosine similarity\n",
    "    scores = [[cosine_similarity(query_vector, document_vectors[d]),d]for d in range(len(document_vectors))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    print('Top-{0} documents'.format(k))\n",
    "    for i in range(k):\n",
    "        print(i, original_documents[scores[i][1]])\n",
    "\n",
    "query = \"tesla nasa\"\n",
    "stemmer = PorterStemmer()\n",
    "search_vec(query, 5, vocabulary, stemmer, document_vectors, original_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the scikit-learn library to do the retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 documents\n",
      "0 b'@ashwin7002 @NASA @faa @AFPAA We have not ruled that out.'\n",
      "1 b\"SpaceX could not do this without NASA. Can't express enough appreciation. https://t.co/uQpI60zAV7\"\n",
      "2 b'@NASA launched a rocket into the northern lights http://t.co/tR2cSeMV'\n",
      "3 b'Whatever happens today, we could not have done it without @NASA, but errors are ours alone and me most of all.'\n",
      "4 b'RT @NASA: Updated @SpaceX #Dragon #ISS rendezvous times: NASA TV coverage begins Sunday at 3:30amET: http://t.co/qrm0Dz4jPE. Grapple at  ...'\n"
     ]
    }
   ],
   "source": [
    "new_features = tfidf.transform([query])\n",
    "\n",
    "cosine_similarities = linear_kernel(new_features, corpus_tf_idf).flatten()\n",
    "related_docs_indices = cosine_similarities.argsort()[::-1]\n",
    "\n",
    "topk = 5\n",
    "print('Top-{0} documents'.format(topk))\n",
    "for i in range(topk):\n",
    "    print(i, original_documents[related_docs_indices[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first NLP exercise is about preprocessing.\n",
    "\n",
    "You will practice preprocessing using NLTK on raw data. \n",
    "This is the first step in most of the NLP projects, so you have to master it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will play with the *coldplay.csv* dataset, containing all the songs and lyrics of Coldplay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know, the first step is to import some libraries. So import *nltk* as well as all the libraries you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ac700dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import NLTK and all the needed libraries\n",
    "import nltk\n",
    "nltk.download('punkt') #Run this line one time to get the resource\n",
    "nltk.download('stopwords') #Run this line one time to get the resource\n",
    "nltk.download('wordnet') #Run this line one time to get the resource\n",
    "nltk.download('averaged_perceptron_tagger') #Run this line one time to get the resource\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f7e68",
   "metadata": {},
   "source": [
    "Load now the dataset using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95636c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Link</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coldplay</td>\n",
       "      <td>Another's Arms</td>\n",
       "      <td>/c/coldplay/anothers+arms_21079526.html</td>\n",
       "      <td>Late night watching tv  \\nUsed to be you here ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coldplay</td>\n",
       "      <td>Bigger Stronger</td>\n",
       "      <td>/c/coldplay/bigger+stronger_20032648.html</td>\n",
       "      <td>I want to be bigger stronger drive a faster ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coldplay</td>\n",
       "      <td>Daylight</td>\n",
       "      <td>/c/coldplay/daylight_20032625.html</td>\n",
       "      <td>To my surprise, and my delight  \\nI saw sunris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coldplay</td>\n",
       "      <td>Everglow</td>\n",
       "      <td>/c/coldplay/everglow_21104546.html</td>\n",
       "      <td>Oh, they say people come  \\nThey say people go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coldplay</td>\n",
       "      <td>Every Teardrop Is A Waterfall</td>\n",
       "      <td>/c/coldplay/every+teardrop+is+a+waterfall_2091...</td>\n",
       "      <td>I turn the music up, I got my records on  \\nI ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Artist                           Song  \\\n",
       "0  Coldplay                 Another's Arms   \n",
       "1  Coldplay                Bigger Stronger   \n",
       "2  Coldplay                       Daylight   \n",
       "3  Coldplay                       Everglow   \n",
       "4  Coldplay  Every Teardrop Is A Waterfall   \n",
       "\n",
       "                                                Link  \\\n",
       "0            /c/coldplay/anothers+arms_21079526.html   \n",
       "1          /c/coldplay/bigger+stronger_20032648.html   \n",
       "2                 /c/coldplay/daylight_20032625.html   \n",
       "3                 /c/coldplay/everglow_21104546.html   \n",
       "4  /c/coldplay/every+teardrop+is+a+waterfall_2091...   \n",
       "\n",
       "                                              Lyrics  \n",
       "0  Late night watching tv  \\nUsed to be you here ...  \n",
       "1  I want to be bigger stronger drive a faster ca...  \n",
       "2  To my surprise, and my delight  \\nI saw sunris...  \n",
       "3  Oh, they say people come  \\nThey say people go...  \n",
       "4  I turn the music up, I got my records on  \\nI ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Load the dataset in coldplay.csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv('coldplay.csv')\n",
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe084b",
   "metadata": {},
   "source": [
    "Now, check the dataset, play with it a bit: what are the columns? How many lines? Is there missing data?..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc148ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120 entries, 0 to 119\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Artist  120 non-null    object\n",
      " 1   Song    120 non-null    object\n",
      " 2   Link    120 non-null    object\n",
      " 3   Lyrics  120 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# TODO: Explore the data\n",
    "df = pd.read_csv('coldplay.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bd15d3",
   "metadata": {},
   "source": [
    "Now select the song 'Every Teardrop Is A Waterfall' and save the Lyrics text into a variable. Print the output of this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "796ecb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    I turn the music up, I got my records on  \\nI ...\n",
      "Name: Lyrics, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# TODO: Select the song 'Every Teardrop Is A Waterfall'\n",
    "df_Song = df[df['Song'] == \"Every Teardrop Is A Waterfall\"]\n",
    "df_Lyrics = df_Song[\"Lyrics\"]\n",
    "print(df_Lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fc6241",
   "metadata": {},
   "source": [
    "As you can see, there is some preprocessing needed here. So let's do it! What is usually the first step?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d45b14",
   "metadata": {},
   "source": [
    "Tokenization, yes. So do tokenization on the lyrics of Every Teardrop Is A Waterfall.\n",
    "\n",
    "So you may have to import the needed library from NLTK if you did not yet.\n",
    "\n",
    "Be careful, the output you have from your pandas dataframe may not have the right type, so manipulate it wisely to get a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a3e7a3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'turn',\n",
       " 'the',\n",
       " 'music',\n",
       " 'up',\n",
       " 'i',\n",
       " 'got',\n",
       " 'my',\n",
       " 'record',\n",
       " 'on',\n",
       " 'i',\n",
       " 'shut',\n",
       " 'the',\n",
       " 'world',\n",
       " 'outsid',\n",
       " 'until',\n",
       " 'the',\n",
       " 'light',\n",
       " 'come',\n",
       " 'on',\n",
       " 'mayb',\n",
       " 'the',\n",
       " 'street',\n",
       " 'alight',\n",
       " 'mayb',\n",
       " 'the',\n",
       " 'tree',\n",
       " 'are',\n",
       " 'gone',\n",
       " 'i',\n",
       " 'feel',\n",
       " 'my',\n",
       " 'heart',\n",
       " 'start',\n",
       " 'beat',\n",
       " 'to',\n",
       " 'my',\n",
       " 'favourit',\n",
       " 'song',\n",
       " 'and',\n",
       " 'all',\n",
       " 'the',\n",
       " 'kid',\n",
       " 'they',\n",
       " 'danc',\n",
       " 'all',\n",
       " 'the',\n",
       " 'kid',\n",
       " 'all',\n",
       " 'night',\n",
       " 'until',\n",
       " 'monday',\n",
       " 'morn',\n",
       " 'feel',\n",
       " 'anoth',\n",
       " 'life',\n",
       " 'i',\n",
       " 'turn',\n",
       " 'the',\n",
       " 'music',\n",
       " 'up',\n",
       " 'im',\n",
       " 'on',\n",
       " 'a',\n",
       " 'roll',\n",
       " 'thi',\n",
       " 'time',\n",
       " 'and',\n",
       " 'heaven',\n",
       " 'is',\n",
       " 'in',\n",
       " 'sight',\n",
       " 'i',\n",
       " 'turn',\n",
       " 'the',\n",
       " 'music',\n",
       " 'up',\n",
       " 'i',\n",
       " 'got',\n",
       " 'my',\n",
       " 'record',\n",
       " 'on',\n",
       " 'from',\n",
       " 'underneath',\n",
       " 'the',\n",
       " 'rubbl',\n",
       " 'sing',\n",
       " 'a',\n",
       " 'rebel',\n",
       " 'song',\n",
       " 'dont',\n",
       " 'want',\n",
       " 'to',\n",
       " 'see',\n",
       " 'anoth',\n",
       " 'gener',\n",
       " 'drop',\n",
       " 'id',\n",
       " 'rather',\n",
       " 'be',\n",
       " 'a',\n",
       " 'comma',\n",
       " 'than',\n",
       " 'a',\n",
       " 'full',\n",
       " 'stop',\n",
       " 'mayb',\n",
       " 'im',\n",
       " 'in',\n",
       " 'the',\n",
       " 'black',\n",
       " 'mayb',\n",
       " 'im',\n",
       " 'on',\n",
       " 'my',\n",
       " 'knee',\n",
       " 'mayb',\n",
       " 'im',\n",
       " 'in',\n",
       " 'the',\n",
       " 'gap',\n",
       " 'between',\n",
       " 'the',\n",
       " 'two',\n",
       " 'trapez',\n",
       " 'but',\n",
       " 'my',\n",
       " 'heart',\n",
       " 'is',\n",
       " 'beat',\n",
       " 'and',\n",
       " 'my',\n",
       " 'puls',\n",
       " 'start',\n",
       " 'cathedr',\n",
       " 'in',\n",
       " 'my',\n",
       " 'heart',\n",
       " 'as',\n",
       " 'we',\n",
       " 'saw',\n",
       " 'oh',\n",
       " 'thi',\n",
       " 'light',\n",
       " 'i',\n",
       " 'swear',\n",
       " 'you',\n",
       " 'emerg',\n",
       " 'blink',\n",
       " 'into',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'it',\n",
       " 'alright',\n",
       " 'as',\n",
       " 'we',\n",
       " 'soar',\n",
       " 'wall',\n",
       " 'everi',\n",
       " 'siren',\n",
       " 'is',\n",
       " 'a',\n",
       " 'symphoni',\n",
       " 'and',\n",
       " 'everi',\n",
       " 'tear',\n",
       " 'a',\n",
       " 'waterfal',\n",
       " 'is',\n",
       " 'a',\n",
       " 'waterfal',\n",
       " 'oh',\n",
       " 'is',\n",
       " 'a',\n",
       " 'waterfal',\n",
       " 'oh',\n",
       " 'oh',\n",
       " 'oh',\n",
       " 'is',\n",
       " 'a',\n",
       " 'is',\n",
       " 'a',\n",
       " 'waterfal',\n",
       " 'everi',\n",
       " 'tear',\n",
       " 'is',\n",
       " 'a',\n",
       " 'waterfal',\n",
       " 'oh',\n",
       " 'oh',\n",
       " 'oh',\n",
       " 'so',\n",
       " 'you',\n",
       " 'can',\n",
       " 'hurt',\n",
       " 'hurt',\n",
       " 'me',\n",
       " 'bad',\n",
       " 'but',\n",
       " 'still',\n",
       " 'ill',\n",
       " 'rais',\n",
       " 'the',\n",
       " 'flag',\n",
       " 'oh',\n",
       " 'it',\n",
       " 'wa',\n",
       " 'a',\n",
       " 'wa',\n",
       " 'wa',\n",
       " 'wa',\n",
       " 'wa',\n",
       " 'waaterfal',\n",
       " 'a',\n",
       " 'wa',\n",
       " 'wa',\n",
       " 'wa',\n",
       " 'wa',\n",
       " 'waaterfal',\n",
       " 'everi',\n",
       " 'tear',\n",
       " 'everi',\n",
       " 'tear',\n",
       " 'everi',\n",
       " 'teardrop',\n",
       " 'is',\n",
       " 'a',\n",
       " 'waterfal',\n",
       " 'everi',\n",
       " 'tear',\n",
       " 'everi',\n",
       " 'tear',\n",
       " 'everi',\n",
       " 'teardrop',\n",
       " 'is',\n",
       " 'a',\n",
       " 'waterfal',\n",
       " 'everi',\n",
       " 'tear',\n",
       " 'everi',\n",
       " 'tear',\n",
       " 'everi',\n",
       " 'teardrop',\n",
       " 'is',\n",
       " 'a',\n",
       " 'waterfal']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Tokenize the lyrics of the song and save the tokens into a variable and print it\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import *\n",
    "\n",
    "def normalize(document):\n",
    "    # TODO: remove punctuation\n",
    "    text = \"\".join([ch for ch in document if ch not in string.punctuation])\n",
    "    \n",
    "    # TODO: tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # TODO: Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    ret = \" \".join([stemmer.stem(word.lower())for word in tokens])\n",
    "    return ret\n",
    "\n",
    "original_documents = [x.strip() for x in df_Song['Lyrics']]\n",
    "documents = [normalize(d).split() for d in original_documents]\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af154c72",
   "metadata": {},
   "source": [
    "It begins to look good. But still, we have the punctuation to remove, so let's do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34e5b3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "import pprint \n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462ff598",
   "metadata": {},
   "source": [
    "We will now remove the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6964a6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'turn',\n",
       "  'the',\n",
       "  'music',\n",
       "  'up',\n",
       "  'i',\n",
       "  'got',\n",
       "  'my',\n",
       "  'record',\n",
       "  'on',\n",
       "  'i',\n",
       "  'shut',\n",
       "  'the',\n",
       "  'world',\n",
       "  'outsid',\n",
       "  'until',\n",
       "  'the',\n",
       "  'light',\n",
       "  'come',\n",
       "  'on',\n",
       "  'mayb',\n",
       "  'the',\n",
       "  'street',\n",
       "  'alight',\n",
       "  'mayb',\n",
       "  'the',\n",
       "  'tree',\n",
       "  'are',\n",
       "  'gone',\n",
       "  'i',\n",
       "  'feel',\n",
       "  'my',\n",
       "  'heart',\n",
       "  'start',\n",
       "  'beat',\n",
       "  'to',\n",
       "  'my',\n",
       "  'favourit',\n",
       "  'song',\n",
       "  'and',\n",
       "  'all',\n",
       "  'the',\n",
       "  'kid',\n",
       "  'they',\n",
       "  'danc',\n",
       "  'all',\n",
       "  'the',\n",
       "  'kid',\n",
       "  'all',\n",
       "  'night',\n",
       "  'until',\n",
       "  'monday',\n",
       "  'morn',\n",
       "  'feel',\n",
       "  'anoth',\n",
       "  'life',\n",
       "  'i',\n",
       "  'turn',\n",
       "  'the',\n",
       "  'music',\n",
       "  'up',\n",
       "  'im',\n",
       "  'on',\n",
       "  'a',\n",
       "  'roll',\n",
       "  'thi',\n",
       "  'time',\n",
       "  'and',\n",
       "  'heaven',\n",
       "  'is',\n",
       "  'in',\n",
       "  'sight',\n",
       "  'i',\n",
       "  'turn',\n",
       "  'the',\n",
       "  'music',\n",
       "  'up',\n",
       "  'i',\n",
       "  'got',\n",
       "  'my',\n",
       "  'record',\n",
       "  'on',\n",
       "  'from',\n",
       "  'underneath',\n",
       "  'the',\n",
       "  'rubbl',\n",
       "  'sing',\n",
       "  'a',\n",
       "  'rebel',\n",
       "  'song',\n",
       "  'dont',\n",
       "  'want',\n",
       "  'to',\n",
       "  'see',\n",
       "  'anoth',\n",
       "  'gener',\n",
       "  'drop',\n",
       "  'id',\n",
       "  'rather',\n",
       "  'be',\n",
       "  'a',\n",
       "  'comma',\n",
       "  'than',\n",
       "  'a',\n",
       "  'full',\n",
       "  'stop',\n",
       "  'mayb',\n",
       "  'im',\n",
       "  'in',\n",
       "  'the',\n",
       "  'black',\n",
       "  'mayb',\n",
       "  'im',\n",
       "  'on',\n",
       "  'my',\n",
       "  'knee',\n",
       "  'mayb',\n",
       "  'im',\n",
       "  'in',\n",
       "  'the',\n",
       "  'gap',\n",
       "  'between',\n",
       "  'the',\n",
       "  'two',\n",
       "  'trapez',\n",
       "  'but',\n",
       "  'my',\n",
       "  'heart',\n",
       "  'is',\n",
       "  'beat',\n",
       "  'and',\n",
       "  'my',\n",
       "  'puls',\n",
       "  'start',\n",
       "  'cathedr',\n",
       "  'in',\n",
       "  'my',\n",
       "  'heart',\n",
       "  'as',\n",
       "  'we',\n",
       "  'saw',\n",
       "  'oh',\n",
       "  'thi',\n",
       "  'light',\n",
       "  'i',\n",
       "  'swear',\n",
       "  'you',\n",
       "  'emerg',\n",
       "  'blink',\n",
       "  'into',\n",
       "  'to',\n",
       "  'tell',\n",
       "  'me',\n",
       "  'it',\n",
       "  'alright',\n",
       "  'as',\n",
       "  'we',\n",
       "  'soar',\n",
       "  'wall',\n",
       "  'everi',\n",
       "  'siren',\n",
       "  'is',\n",
       "  'a',\n",
       "  'symphoni',\n",
       "  'and',\n",
       "  'everi',\n",
       "  'tear',\n",
       "  'a',\n",
       "  'waterfal',\n",
       "  'is',\n",
       "  'a',\n",
       "  'waterfal',\n",
       "  'oh',\n",
       "  'is',\n",
       "  'a',\n",
       "  'waterfal',\n",
       "  'oh',\n",
       "  'oh',\n",
       "  'oh',\n",
       "  'is',\n",
       "  'a',\n",
       "  'is',\n",
       "  'a',\n",
       "  'waterfal',\n",
       "  'everi',\n",
       "  'tear',\n",
       "  'is',\n",
       "  'a',\n",
       "  'waterfal',\n",
       "  'oh',\n",
       "  'oh',\n",
       "  'oh',\n",
       "  'so',\n",
       "  'you',\n",
       "  'can',\n",
       "  'hurt',\n",
       "  'hurt',\n",
       "  'me',\n",
       "  'bad',\n",
       "  'but',\n",
       "  'still',\n",
       "  'ill',\n",
       "  'rais',\n",
       "  'the',\n",
       "  'flag',\n",
       "  'oh',\n",
       "  'it',\n",
       "  'wa',\n",
       "  'a',\n",
       "  'wa',\n",
       "  'wa',\n",
       "  'wa',\n",
       "  'wa',\n",
       "  'waaterfal',\n",
       "  'a',\n",
       "  'wa',\n",
       "  'wa',\n",
       "  'wa',\n",
       "  'wa',\n",
       "  'waaterfal',\n",
       "  'everi',\n",
       "  'tear',\n",
       "  'everi',\n",
       "  'tear',\n",
       "  'everi',\n",
       "  'teardrop',\n",
       "  'is',\n",
       "  'a',\n",
       "  'waterfal',\n",
       "  'everi',\n",
       "  'tear',\n",
       "  'everi',\n",
       "  'tear',\n",
       "  'everi',\n",
       "  'teardrop',\n",
       "  'is',\n",
       "  'a',\n",
       "  'waterfal',\n",
       "  'everi',\n",
       "  'tear',\n",
       "  'everi',\n",
       "  'tear',\n",
       "  'everi',\n",
       "  'teardrop',\n",
       "  'is',\n",
       "  'a',\n",
       "  'waterfal']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Remove the punctuation, then save the result into a variable and print it\n",
    "words = [word for word in documents if word not in stopwords.words('english')]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d5be5",
   "metadata": {},
   "source": [
    "Okay we begin to have much less words in our song, right?\n",
    "\n",
    "Next step is lemmatization. But we had an issue in the lectures, you remember? Let's learn how to do it properly now.\n",
    "\n",
    "First let's try to do it naively. Import the WordNetLemmatizer and perform lemmatization with default options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a34c0e",
   "metadata": {},
   "source": [
    "TODO: Perform lemmatization using WordNetLemmatizer on our tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e63e32c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('wordnet')  # Download the WordNet corpus\n",
    "nltk.download('stopwords')  # Download the stopwords corpus\n",
    "nltk.download('omw-1.4')  # Download the omw-1.4 resource\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "filtered_tokens_Array = []\n",
    "\n",
    "for item in documents:\n",
    "    words = [lemmatizer.lemmatize(word) for word in item]\n",
    "    filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "    filtered_tokens_Array.append(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76c40929",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "filtered:\n",
      " [['turn', 'music', 'got', 'record', 'shut', 'world', 'outsid', 'light', 'come', 'mayb', 'street', 'alight', 'mayb', 'tree', 'gone', 'feel', 'heart', 'start', 'beat', 'favourit', 'song', 'kid', 'danc', 'kid', 'night', 'monday', 'morn', 'feel', 'anoth', 'life', 'turn', 'music', 'im', 'roll', 'thi', 'time', 'heaven', 'sight', 'turn', 'music', 'got', 'record', 'underneath', 'rubbl', 'sing', 'rebel', 'song', 'dont', 'want', 'see', 'anoth', 'gener', 'drop', 'id', 'rather', 'comma', 'full', 'stop', 'mayb', 'im', 'black', 'mayb', 'im', 'knee', 'mayb', 'im', 'gap', 'two', 'trapez', 'heart', 'beat', 'pul', 'start', 'cathedr', 'heart', 'saw', 'oh', 'thi', 'light', 'swear', 'emerg', 'blink', 'tell', 'alright', 'soar', 'wall', 'everi', 'siren', 'symphoni', 'everi', 'tear', 'waterfal', 'waterfal', 'oh', 'waterfal', 'oh', 'oh', 'oh', 'waterfal', 'everi', 'tear', 'waterfal', 'oh', 'oh', 'oh', 'hurt', 'hurt', 'bad', 'still', 'ill', 'rais', 'flag', 'oh', 'wa', 'wa', 'wa', 'wa', 'wa', 'waaterfal', 'wa', 'wa', 'wa', 'wa', 'waaterfal', 'everi', 'tear', 'everi', 'tear', 'everi', 'teardrop', 'waterfal', 'everi', 'tear', 'everi', 'tear', 'everi', 'teardrop', 'waterfal', 'everi', 'tear', 'everi', 'tear', 'everi', 'teardrop', 'waterfal']]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nfiltered:\\n\",filtered_tokens_Array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8dbee8",
   "metadata": {},
   "source": [
    "Okay we begin to have much less words in our song, right?\n",
    "\n",
    "Next step is lemmatization. But we had an issue in the lectures, you remember? Let's learn how to do it properly now.\n",
    "\n",
    "First let's try to do it naively. Import the WordNetLemmatizer and perform lemmatization with default options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8523e8",
   "metadata": {},
   "source": [
    "TODO: use the function pos_tag of NLTK to perform POS-tagging and print the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffaf4d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34bfec57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('turn', 'NOUN'), ('music', 'NOUN'), ('got', 'VERB'), ('record', 'NOUN'), ('shut', 'NOUN'), ('world', 'NOUN'), ('outsid', 'NOUN'), ('light', 'ADJ'), ('come', 'VERB'), ('mayb', 'ADJ'), ('street', 'NOUN'), ('alight', 'VERB'), ('mayb', 'NOUN'), ('tree', 'VERB'), ('gone', 'VERB'), ('feel', 'ADJ'), ('heart', 'NOUN'), ('start', 'NOUN'), ('beat', 'VERB'), ('favourit', 'ADJ'), ('song', 'NOUN'), ('kid', 'NOUN'), ('danc', 'NOUN'), ('kid', 'NOUN'), ('night', 'NOUN'), ('monday', 'NOUN'), ('morn', 'VERB'), ('feel', 'VERB'), ('anoth', 'ADJ'), ('life', 'NOUN'), ('turn', 'NOUN'), ('music', 'NOUN'), ('im', 'NOUN'), ('roll', 'NOUN'), ('thi', 'NOUN'), ('time', 'NOUN'), ('heaven', 'ADJ'), ('sight', 'VERB'), ('turn', 'NOUN'), ('music', 'NOUN'), ('got', 'VERB'), ('record', 'ADJ'), ('underneath', 'NOUN'), ('rubbl', 'NOUN'), ('sing', 'VERB'), ('rebel', 'NOUN'), ('song', 'NOUN'), ('dont', 'NOUN'), ('want', 'VERB'), ('see', 'VERB'), ('anoth', 'DET'), ('gener', 'X'), ('drop', 'NOUN'), ('id', 'NOUN'), ('rather', 'ADV'), ('comma', 'ADJ'), ('full', 'ADJ'), ('stop', 'NOUN'), ('mayb', 'NOUN'), ('im', 'ADJ'), ('black', 'ADJ'), ('mayb', 'NOUN'), ('im', 'NOUN'), ('knee', 'NOUN'), ('mayb', 'NOUN'), ('im', 'ADJ'), ('gap', 'NOUN'), ('two', 'NUM'), ('trapez', 'ADJ'), ('heart', 'NOUN'), ('beat', 'NOUN'), ('pul', 'ADJ'), ('start', 'NOUN'), ('cathedr', 'VERB'), ('heart', 'NOUN'), ('saw', 'NOUN'), ('oh', 'ADJ'), ('thi', 'NOUN'), ('light', 'ADJ'), ('swear', 'ADJ'), ('emerg', 'ADJ'), ('blink', 'NOUN'), ('tell', 'NOUN'), ('alright', 'ADV'), ('soar', 'VERB'), ('wall', 'NOUN'), ('everi', 'NOUN'), ('siren', 'NOUN'), ('symphoni', 'VERB'), ('everi', 'ADJ'), ('tear', 'ADJ'), ('waterfal', 'NOUN'), ('waterfal', 'NOUN'), ('oh', 'X'), ('waterfal', 'NOUN'), ('oh', 'ADJ'), ('oh', 'ADJ'), ('oh', 'NOUN'), ('waterfal', 'NOUN'), ('everi', 'VERB'), ('tear', 'ADJ'), ('waterfal', 'NOUN'), ('oh', 'VERB'), ('oh', 'VERB'), ('oh', 'ADJ'), ('hurt', 'VERB'), ('hurt', 'NOUN'), ('bad', 'ADJ'), ('still', 'ADV'), ('ill', 'ADJ'), ('rais', 'ADJ'), ('flag', 'NOUN'), ('oh', 'ADP'), ('wa', 'ADJ'), ('wa', 'NOUN'), ('wa', 'NOUN'), ('wa', 'NOUN'), ('wa', 'NOUN'), ('waaterfal', 'NOUN'), ('wa', 'NOUN'), ('wa', 'NOUN'), ('wa', 'NOUN'), ('wa', 'NOUN'), ('waaterfal', 'NOUN'), ('everi', 'VERB'), ('tear', 'ADJ'), ('everi', 'ADJ'), ('tear', 'ADJ'), ('everi', 'NOUN'), ('teardrop', 'NOUN'), ('waterfal', 'NOUN'), ('everi', 'VERB'), ('tear', 'ADJ'), ('everi', 'ADJ'), ('tear', 'ADJ'), ('everi', 'NOUN'), ('teardrop', 'NOUN'), ('waterfal', 'NOUN'), ('everi', 'VERB'), ('tear', 'ADJ'), ('everi', 'ADJ'), ('tear', 'ADJ'), ('everi', 'NOUN'), ('teardrop', 'NOUN'), ('waterfal', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(filtered_words, tagset=\"universal\")\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b649d90",
   "metadata": {},
   "source": [
    "As you can see, it worked well on nouns (plural words are now singular for example).\n",
    "\n",
    "But verbs are not OK: we would 'is' to become 'be' for example.\n",
    "\n",
    "To do that, we need to do POS-tagging. So let's do this now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaa9aee",
   "metadata": {},
   "source": [
    "POS-tagging means Part of speech tagging: basically it will classify words into categories: like verbs, nouns, advers and so on...\n",
    "\n",
    "In order to do that, we will use NLTK and the function *pos_tag*. You have to do it on the step before lemmatization, so use your variable containing all the tokens without punctuation and without stop words.\n",
    "\n",
    "Hint: you can check on the internet how the *pos_tag* function works [here](https://www.nltk.org/book/ch05.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219f786d",
   "metadata": {},
   "source": [
    " TODO: use the function pos_tag of NLTK to perform POS-tagging and print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('turn', 'VBP'), ('music', 'NN'), ('got', 'VBD'), ('records', 'NNS'), ('shut', 'VBD'), ('world', 'NN'), ('lights', 'NNS'), ('come', 'VBP'), ('streets', 'NNS'), ('alight', 'VBD'), ('trees', 'NNS'), ('are', 'VBP'), ('gone', 'VBN'), ('feel', 'VBP'), ('heart', 'NN'), ('start', 'VB'), ('beating', 'NN'), ('song', 'NN'), ('all', 'PDT'), ('kids', 'NNS'), ('dance', 'VBP'), ('all', 'PDT'), ('kids', 'NNS'), ('night', 'NN'), ('Monday', 'NNP'), ('morning', 'NN'), ('feels', 'NNS'), ('life', 'NN'), ('turn', 'VBP'), ('music', 'NN'), (\"'m\", 'VBP'), ('roll', 'NN'), ('time', 'NN'), ('heaven', 'NN'), ('sight', 'NN'), ('turn', 'VBP'), ('music', 'NN'), ('got', 'VBD'), ('records', 'NNS'), ('rubble', 'NN'), ('sing', 'VBG'), ('rebel', 'NN'), ('song', 'NN'), ('Do', 'VBP'), ('want', 'VB'), ('see', 'VB'), ('generation', 'NN'), ('drop', 'NN'), ('be', 'VB'), ('comma', 'NN'), ('stop', 'NN'), (\"'m\", 'VBP'), (\"'m\", 'VBP'), ('knees', 'NNS'), (\"'m\", 'VBP'), ('gap', 'NN'), ('trapezes', 'NNS'), ('heart', 'NN'), ('beating', 'VBG'), ('pulses', 'NNS'), ('start', 'VBP'), ('Cathedrals', 'NNS'), ('heart', 'NN'), ('saw', 'VBD'), ('swear', 'VBP'), (\"'m\", 'VBP'), ('gon', 'VBG'), ('give', 'VB'), ('try', 'NN'), ('leave', 'VB'), ('door', 'NN'), (\"'m\", 'VBP'), ('gon', 'VBG'), ('be', 'VB'), ('one', 'NN'), ('who', 'WP'), (\"'m\", 'VBP'), ('gon', 'VBG'), ('be', 'VB'), ('one', 'NN'), ('who', 'WP'), ('gon', 'NN'), ('let', 'VB'), (\"'m\", 'VBP'), ('gon', 'VBG'), ('be', 'VB'), ('one', 'NN'), ('who', 'WP'), (\"'m\", 'VBP'), ('gon', 'VBG'), ('be', 'VB'), ('one', 'NN'), ('who', 'WP'), ('gon', 'NN'), ('let', 'VB'), ('hurt', 'VBP'), ('mind', 'VB'), ('raise', 'VB'), ('flag', 'NN'), ('was', 'VBD'), ('wa', 'NN'), ('wa', 'NN'), ('wa', 'NN'), ('wa', 'NN'), ('wa', 'NN'), ('was', 'VBD'), ('wa', 'NN'), ('wa', 'NN'), ('wa', 'NN'), ('wa', 'NN'), ('wa', 'NN'), ('wa', 'NN'), ('tear', 'NN'), ('tear', 'NN'), ('teardrop', 'NN'), ('waterfall', 'NN'), ('was', 'VBD'), ('wa', 'NN'), ('wa', 'NN'), ('wa', 'NN'), ('wa', 'NN'), ('wa', 'NN'), ('tear', 'NN'), ('waterfall', 'NN'), ('hurt', 'VBP'), ('mind', 'VB'), ('raise', 'VB'), ('flag', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "sentence = \"I turn the music on, I got my records on. I shut the world outside until the lights come on. Maybe the streets alight, maybe the trees are gone. I feel my heart start beating to my favorite song. And all the kids, they dance, all the kids all night. Until Monday morning feels another life. I turn the music up, I'm on a roll this time. And heaven is in sight. I turn the music up, I got my records on. From underneath the rubble, sing a rebel song. Don't want to see another generation drop. I'd rather be a comma than a full stop. Maybe I'm in the black, maybe I'm on my knees. Maybe I'm in the gap between the two trapezes. But my heart is beating and my pulses start. Cathedrals in my heart. As we saw oh, this light I swear. Oh, I'm gonna give it a try, I'll leave my door open wide. And I'm gonna be the one who never lets you in. And I'm gonna be the one who's gonna let you in. I'm gonna be the one who never lets you in. I'm gonna be the one who's gonna let you in. So, you hurt me bad but I won't mind. Still, I'll raise the flag. Oh, It was a wa wa wa wa wa wa. It was a wa wa wa wa wa wa wa. Every tear, every tear. Every teardrop is a waterfall. Oh, It was a wa wa wa wa wa wa. Every tear is a waterfall. So, you hurt me bad, but I won't mind. Still, I'll raise the flag. Oh.\"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "tagged_words = pos_tag(tokens)\n",
    "\n",
    "result = [(word, tag) for word, tag in tagged_words if tag not in [\"DT\", \"IN\", \"CC\", \"RB\", \"TO\", \"PRP\", \"PRP$\", \"MD\", \"JJ\", \"CD\", \"UH\", \".\", \",\", \"VBZ\"]]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it does not return values like 'a', 'n', 'v' or 'r' as the WordNet lemmatizer is expecting...\n",
    "\n",
    "So we have to convert the values from the NLTK POS-tagging to put them into the WordNet Lemmatizer. This is done in the function *get_wordnet_pos* written below. Try to understand it, and then we will reuse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    output = np.asarray(pos_tag)\n",
    "    for i in range(len(pos_tag)):\n",
    "        if pos_tag[i][1].startswith('J'):\n",
    "            output[i][1] = wordnet.ADJ\n",
    "        elif pos_tag[i][1].startswith('V'):\n",
    "            output[i][1] = wordnet.VERB\n",
    "        elif pos_tag[i][1].startswith('R'):\n",
    "            output[i][1] = wordnet.ADV\n",
    "        else:\n",
    "            output[i][1] = wordnet.NOUN\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now you have all we need to perform properly the lemmatization.\n",
    "\n",
    "So you have to use the following to do so:\n",
    "* your tags from the POS-tagging performed\n",
    "* the function *get_wordnet_pos*\n",
    "* the *WordNetLemmatizer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the lemmatization properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'turn', 'music', 'I', 'get', 'record', 'I', 'shut', 'world', 'outside', 'light', 'come', 'Maybe', 'street', 'alight', 'maybe', 'tree', 'go', 'I', 'feel', 'heart', 'start', 'beat', 'favourite', 'song', 'And', 'kid', 'dance', 'kid', 'night', 'Until', 'Monday', 'morning', 'feel', 'another', 'life', 'I', 'turn', 'music', 'I', 'roll', 'time', 'And', 'heaven', 'sight', 'I', 'turn', 'music', 'I', 'get', 'record', 'From', 'underneath', 'rubble', 'sing', 'rebel', 'song', 'Do', 'want', 'see', 'another', 'generation', 'drop', 'I', 'rather', 'comma', 'full', 'stop', 'Maybe', 'I', 'black', 'maybe', 'I', 'knee', 'Maybe', 'I', 'gap', 'two', 'trapeze', 'But', 'heart', 'beating', 'pulse', 'start', 'Cathedrals', 'heart', 'As', 'saw', 'oh', 'light', 'I', 'swear', 'emerge', 'blink', 'To', 'tell', 'alright', 'As', 'soar', 'wall', 'every', 'siren', 'symphony', 'And', 'every', 'tear', 'waterfall', 'Is', 'waterfall', 'Oh', 'Is', 'waterfall', 'Oh', 'oh', 'oh', 'Is', 'waterfall', 'Every', 'tear', 'Is', 'waterfall', 'Oh', 'oh', 'oh', 'So', 'hurt', 'hurt', 'bad', 'But', 'still', 'I', 'raise', 'flag', 'Oh', 'It', 'wa', 'wa', 'wa', 'wa', 'A', 'wa', 'wa', 'wa', 'wa', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'waterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'waterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'waterfall']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Input text\n",
    "words = ['I', 'turn', 'music', 'I', 'get', 'record', 'I', 'shut', 'world', 'outside', 'light', 'come', 'Maybe', 'street',\n",
    "         'alight', 'maybe', 'tree', 'go', 'I', 'feel', 'heart', 'start', 'beat', 'favourite', 'song', 'And', 'kid',\n",
    "         'dance', 'kid', 'night', 'Until', 'Monday', 'morning', 'feel', 'another', 'life', 'I', 'turn', 'music', 'I',\n",
    "         'roll', 'time', 'And', 'heaven', 'sight', 'I', 'turn', 'music', 'I', 'get', 'record', 'From', 'underneath',\n",
    "         'rubble', 'sing', 'rebel', 'song', 'Do', 'want', 'see', 'another', 'generation', 'drop', 'I', 'rather', 'comma',\n",
    "         'full', 'stop', 'Maybe', 'I', 'black', 'maybe', 'I', 'knees', 'Maybe', 'I', 'gap', 'two', 'trapeze', 'But',\n",
    "         'heart', 'beating', 'pulse', 'start', 'Cathedrals', 'heart', 'As', 'saw', 'oh', 'light', 'I', 'swear',\n",
    "         'emerge', 'blink', 'To', 'tell', 'alright', 'As', 'soar', 'wall', 'every', 'siren', 'symphony', 'And', 'every',\n",
    "         'tear', 'waterfall', 'Is', 'waterfall', 'Oh', 'Is', 'waterfall', 'Oh', 'oh', 'oh', 'Is', 'waterfall', 'Every',\n",
    "         'tear', 'Is', 'waterfall', 'Oh', 'oh', 'oh', 'So', 'hurt', 'hurt', 'bad', 'But', 'still', 'I', 'raise', 'flag',\n",
    "         'Oh', 'It', 'wa', 'wa', 'wa', 'wa', 'A', 'wa', 'wa', 'wa', 'wa', 'Every', 'tear', 'Every', 'tear', 'Every',\n",
    "         'teardrop', 'waterfall', 'Every', 'tear', 'Every', 'tear', 'Every', 'teardrop', 'waterfall', 'Every', 'tear',\n",
    "         'Every', 'tear', 'Every', 'teardrop', 'waterfall']\n",
    "\n",
    "# Lemmatize each word\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "# Print the lemmatized words\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think?\n",
    "\n",
    "Still not perfect, but it's the best we can do for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can try stemming, with the help of the lecture, and see the differences compared to the lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'turn', 'music', 'i', 'got', 'record', 'i', 'shut', 'world', 'outsid', 'light', 'come', 'mayb', 'street', 'alight', 'mayb', 'tree', 'gone', 'i', 'feel', 'heart', 'start', 'beat', 'favourit', 'song', 'and', 'kid', 'danc', 'kid', 'night', 'until', 'monday', 'morn', 'feel', 'anoth', 'life', 'i', 'turn', 'music', 'i', 'roll', 'time', 'and', 'heaven', 'sight', 'i', 'turn', 'music', 'i', 'got', 'record', 'from', 'underneath', 'rubbl', 'sing', 'rebel', 'song', 'do', 'want', 'see', 'anoth', 'gener', 'drop', 'i', 'rather', 'comma', 'full', 'stop', 'mayb', 'i', 'black', 'mayb', 'i', 'knee', 'mayb', 'i', 'gap', 'two', 'trapez', 'but', 'heart', 'beat', 'pul', 'start', 'cathedr', 'heart', 'as', 'saw', 'oh', 'light', 'i', 'swear', 'emerg', 'blink', 'to', 'tell', 'alright', 'as', 'soar', 'wall', 'everi', 'siren', 'symphoni', 'and', 'everi', 'tear', 'waterf', 'is', 'waterf', 'oh', 'is', 'waterf', 'oh', 'oh', 'oh', 'is', 'waterf', 'everi', 'tear', 'is', 'waterf', 'oh', 'oh', 'oh', 'so', 'hurt', 'hurt', 'bad', 'but', 'still', 'i', 'rai', 'flag', 'oh', 'it', 'wa', 'wa', 'wa', 'wa', 'a', 'wa', 'wa', 'wa', 'wa', 'everi', 'tear', 'everi', 'tear', 'everi', 'teardrop', 'waterf', 'everi', 'tear', 'everi', 'tear', 'everi', 'teardrop', 'waterf', 'everi', 'tear', 'everi', 'tear', 'everi', 'teardrop', 'waterf']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def perform_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = []\n",
    "\n",
    "    for word in text:\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        stemmed_words.append(stemmed_word)\n",
    "\n",
    "    return stemmed_words\n",
    "\n",
    "# Input text\n",
    "text = ['i', 'turn', 'music', 'i', 'got', 'record', 'i', 'shut', 'world', 'outsid', 'light', 'come', 'mayb', 'street', 'alight', 'mayb', 'tree', 'gone', 'i', 'feel', 'heart', 'start', 'beat', 'favourit', 'song', 'and', 'kid', 'danc', 'kid', 'night', 'until', 'monday', 'morn', 'feel', 'anoth', 'life', 'i', 'turn', 'music', 'i', 'roll', 'time', 'and', 'heaven', 'sight', 'i', 'turn', 'music', 'i', 'got', 'record', 'from', 'underneath', 'rubbl', 'sing', 'rebel', 'song', 'do', 'want', 'see', 'anoth', 'gener', 'drop', 'i', 'rather', 'comma', 'full', 'stop', 'mayb', 'i', 'black', 'mayb', 'i', 'knee', 'mayb', 'i', 'gap', 'two', 'trapez', 'but', 'heart', 'beat', 'puls', 'start', 'cathedr', 'heart', 'as', 'saw', 'oh', 'light', 'i', 'swear', 'emerg', 'blink', 'to', 'tell', 'alright', 'as', 'soar', 'wall', 'everi', 'siren', 'symphoni', 'and', 'everi', 'tear', 'waterfal', 'is', 'waterfal', 'oh', 'is', 'waterfal', 'oh', 'oh', 'oh', 'is', 'waterfal', 'everi', 'tear', 'is', 'waterfal', 'oh', 'oh', 'oh', 'so', 'hurt', 'hurt', 'bad', 'but', 'still', 'i', 'rais', 'flag', 'oh', 'it', 'wa', 'wa', 'wa', 'wa', 'a', 'wa', 'wa', 'wa', 'wa', 'everi', 'tear', 'everi', 'tear', 'everi', 'teardrop', 'waterfal', 'everi', 'tear', 'everi', 'tear', 'everi', 'teardrop', 'waterfal', 'everi', 'tear', 'everi', 'tear', 'everi', 'teardrop', 'waterfal']\n",
    "\n",
    "# Perform stemming\n",
    "stemmed_text = perform_stemming(text)\n",
    "\n",
    "# Print the result\n",
    "print(stemmed_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
